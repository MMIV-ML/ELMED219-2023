{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMED219 Lab1 BRATS 3D fastMONAI v2022-12-29\n",
    "\n",
    "# Brain tumor 3D segmentation with fastMONAI\n",
    "\n",
    "This notebook is essentially a copy of the multi-class semantic segmentation notebook `10d_tutorial_multiclass_segmentation.ipynb` from [fastMONAI](https://github.com/MMIV-ML/fastMONAI). We will not have time for a detailed introduction to MONAI. Please consult the documentation: https://monai.io and also see their [tutorials](https://github.com/Project-MONAI/tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Notebook    |      1-Click Notebook      |\n",
    "|:----------|------|\n",
    "|  [ELMED219-Lab1-BRATS-3D-fastMONAI.ipynb](https://nbviewer.jupyter.org/github/MMIV-ML/ELMED219/blob/main/Lab1-mpMRI-glioblastoma/ELMED219-Lab1-BRATS-3D-fastMONAI.ipynb)<br> The present notebook   | [![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MMIV-ML/ELMED219/blob/main/Lab-optional-imaging/01-imaging-intro.ipynb)|\n",
    "|  [10d_tutorial_multiclass_segmentation.ipynb](https://nbviewer.jupyter.org/github/MMIV-ML/fastMONAI/blob/main/nbs/10d_tutorial_multiclass_segmentation.ipynb)<br> fastMONAI tutoral on multi-class semantic segmentation   | [![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MMIV-ML/fastMONAI/blob/master/nbs/10d_tutorial_multiclass_segmentation.ipynb)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial notebook shows how to construct a training workflow of a **multi-labels segmentation task**:<br>\n",
    "\n",
    "_Obtain pre-defined biologically/clinically significant subregions of a brain tumor from multiparametric 3D MRI recordings using supervised learning with 3D convolutional deep neural networks_, i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multilabel tumor segmentation:  $y  \\approx f(X; \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data $X$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we are using multiparametric magnetic resonance images ([mp-MRI](https://en.wikipedia.org/wiki/MRI_sequence)) recordings from previous BraTS challenges openly available in the **Medical Segmentation Decathlon** ([MSD](https://arxiv.org/abs/2106.05735)) repository (http://medicaldecathlon.com) as `Task01_BrainTumor.tar` (see also https://decathlon-10.grand-challenge.org).\n",
    "\n",
    "The data set consists of 750 mp-MRI recordings from patients diagnosed with either glioblastoma or lower-grade glioma (LGG). The sequences used were native\n",
    "T1-weighted (T1), post-Gadolinium (Gd) contrast T1-weighted (T1-Gd), native T2-weighted (T2),\n",
    "and T2 Fluid-Attenuated Inversion Recovery (FLAIR). The corresponding target ROIs were the\n",
    "three tumor sub-regions, namely edema, enhancing, and non-enhancing tumor. This data set was\n",
    "selected due to the challenge of locating these complex and heterogeneously-located targets. The\n",
    "data was acquired from 19 different institutions and contained a subset of the data used in the\n",
    "[2016](https://www.smir.ch/BRATS/Start2016) and [here](https://paperswithcode.com/dataset/brats-2016) and [2017](https://www.med.upenn.edu/sbia/brats2017.html) Brain Tumor Segmentation (BraTS) challenges [12, 13, 14].\n",
    "\n",
    "There are a total of 750 4D volumes available, 484 four-channel MRI images (`imagesTr` folder) with corresponding multi-label images (`labelsTr`) available for training, and 266 mpMRI recordings (no corresponding labels provided) in the `ìmagesTs`folder. The provided data are distributed after their pre-processing, i.e. co-registered to the same anatomical template, interpolated to the same resolution (1 mm$^3$) and skull-stripped. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels and classes $y$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge we have the following **labels** for the data:\n",
    "  - label 0 is non-tumor\n",
    "  - label 1 is the peritumoral edema\n",
    "  - label 2 is the necrotic and non-enhancing tumor core\n",
    "  - label 3 is the GD-enhancing tumor\n",
    "\n",
    "The possible **classes** are:\n",
    "  - TC (Tumor core)\n",
    "  - WT (Whole tumor)\n",
    "  - ET (Enhancing tumor)\n",
    "\n",
    "(merging label 2 and label 3 to construct TC, merging labels 1, 2 and 3 to construct WT, and label 3 is ET)\n",
    "\n",
    "![img](./assets/ELMED219-Lab1-BraTS-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model $f$ and trainable parameters $\\theta$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our $y  \\approx f(X; \\theta)$ notation we will for each voxel location $v \\in \\Omega(X)$ (spatial domain of the multiparametric brain image $X$) have\n",
    "\n",
    "\n",
    "- $y_v \\in \\{y_1, y_2, y_3, y_4\\}$ = {_TC_, _WT_, _ET_, non-tumor} (these regions are illustrated in the figure below)\n",
    "\n",
    "\n",
    "- $f$ is a 3D convolutional deep neural net model (i.e. `SegResNet`), and \n",
    "\n",
    "\n",
    "- $\\theta = (\\theta_1, \\theta_2, ...., \\theta_p)$ is the $p$ trainable model parameters (``synaptic weights\") in $f$ (in our `SegResNet` model $p \\sim 4.7 \\cdot 10^6$), and\n",
    "\n",
    "\n",
    "- $\\approx$ denotes _approximation_ expressed by a loss function or performance metric such as the Dice coefficient for the segmentation, e.g. accross one epoch during training: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it contains the following steps:\n",
    "1. Transforms for dictionary format data.\n",
    "1. Define a new transform according to MONAI transform API.\n",
    "1. Load Nifti image with metadata, load a list of images and stack them.\n",
    "1. Randomly adjust intensity for data augmentation.\n",
    "1. Cache IO and transforms to accelerate training and validation.\n",
    "1. 3D SegResNet model, Dice loss function, Mean Dice metric for 3D segmentation task.\n",
    "1. Deterministic training for reproducibility.\n",
    "\n",
    "The dataset comes from http://medicaldecathlon.com/.  \n",
    "Target: Gliomas segmentation necrotic/active tumour and oedema  \n",
    "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
    "Size: 750 4D volumes (484 Training + 266 Testing)  \n",
    "Source: BRATS 2016 and 2017 datasets.  \n",
    "Challenge: Complex and heterogeneously-located targets\n",
    "\n",
    "Below figure shows image patches with the tumor sub-regions that are annotated in the different modalities (top left) and the final labels for the whole dataset (right).\n",
    "(Figure taken from the [BraTS IEEE TMI paper](https://ieeexplore.ieee.org/document/6975210/))\n",
    "\n",
    "![image](./assets/brats_tasks.png)\n",
    "\n",
    "The image patches show from left to right:\n",
    "1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).\n",
    "1. the tumor core (red) visible in T2 (Fig.B).\n",
    "1. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).\n",
    "1. The segmentations are combined to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on BraTS 2021\n",
    "\n",
    "- **[NVIDIA Data Scientists](https://developer.nvidia.com/blog/nvidia-data-scientists-take-top-spots-in-miccai-2021-brain-tumor-segmentation-challenge) Take Top Spots in MICCAI 2021 [Brain Tumor Segmentation Challenge](http://www.braintumorsegmentation.org)**:\n",
    "\n",
    "\n",
    " - Optimized U-Net for Brain Tumor Segmentation – Rank #1 (based on [`nnU-Net`](https://github.com/MIC-DKFZ/nnUNet) winner of [BraTS 2020](https://www.med.upenn.edu/cbica/brats2020) by [Isense et al.](https://arxiv.org/abs/2011.00848))\n",
    "  \n",
    "  \n",
    " - SegResNet: Redundancy Reduction in Semantic Segmentation of 3D Brain MRIs – Rank #2<br> The main model is the SegResNet architecture from MONAI, a standard encoder-decoder based convolutional neural network (CNN) similar to U-Net. \n",
    "  \n",
    "![SegResNet](https://developer-blogs.nvidia.com/wp-content/uploads/2021/09/BRaTS-fig-2.png)\n",
    "  \n",
    "_A typical segmentation example with predicted labels overlaid overT1c MRI axial, sagittal and coronal slices. The whole tumor (WT) class includes all visible labels (a union of green, yellow and red labels), the tumor core (TC) class is a union of red and yellow, and the enhancing tumor core (ET) class is shown in yellow\n",
    "(a hyperactive tumor part)._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
